{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルストアでの利用\n",
    "1. FAISSに格納\n",
    "2. FAISSから取得\n",
    "3. FAISSでベクトル検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.vectorstores import Pinecone # ほかのベクトルストアもコード変更なく同様に使える\n",
    "from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⼤規模⾔語モデル\n",
      "出典 : フリー百科事典『ウィキペディア（ Wikipedia ）』\n",
      "⼤規模⾔語モデル（だいきぼげんごモデル、英: large language model、LLM）は、多数のパラメ\n"
     ]
    }
   ],
   "source": [
    "# 1. PDFデータ読み込み\n",
    "pdf_page = PdfReader(\"./data/llm.pdf\")\n",
    "text = \"\"\n",
    "# PDFデータをテキストに変換\n",
    "for page in pdf_page.pages:\n",
    "    text += page.extract_text()\n",
    "print(text[:100]) # 先頭100文字を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docサイズ 64\n",
      "型 <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# 2. データをチャンクに小分けにする\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # チャンクの最大文字数\n",
    "    chunk_overlap=100   # チャンク間の重複文字数\n",
    ")\n",
    "docs = text_splitter.split_text(text) # テキストをチャンクに分割\n",
    "docs_chunks = text_splitter.create_documents(docs) # チャンクをドキュメントに変換\n",
    "print(\"Docサイズ\", len(docs_chunks))\n",
    "print(\"型\", type(docs_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OpenAIのembeddingモデル取得\n",
    "openAI_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "openAI_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "embeddings = None\n",
    "if openAI_key != \"\" :\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "      api_key=openAI_key,\n",
    "      azure_deployment=\"text-embedding-ada-002\",\n",
    "      # openai_api_versiton=\"2024-10-21\",\n",
    "      azure_endpoint=openAI_endpoint\n",
    "    )\n",
    "else:\n",
    "    print(\"EmbeddingのAPIKeyを設定してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "documents 抽出: 100%|██████████| 64/64 [00:06<00:00,  9.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. ベクトル化\n",
    "# 一括でベクトル化する場合\n",
    "# faiss_db = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "# プログレスバーを表示しながらベクトル化\n",
    "faiss_db = None\n",
    "with tqdm(total=len(docs_chunks), desc=\"documents 抽出\") as pbar:\n",
    "    for d in docs_chunks:\n",
    "        if faiss_db:\n",
    "            faiss_db.add_documents([d])\n",
    "        else:\n",
    "            faiss_db = FAISS.from_documents([d], embeddings)\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ベクトル化データを保存\n",
    "faiss_db.save_local(\"./db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル化データを読み込み\n",
    "vectorstore  = FAISS.load_local(\n",
    "    \"./db\", # ベクトル化データの保存先\n",
    "    embeddings, # ベクトル化モデル\n",
    "    allow_dangerous_deserialization=True # デシリアライズを許可 (デフォルトはFalse)でセキュリティを向上\n",
    ")\n",
    "\n",
    "# retrieverを取得\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='⼤規模⾔語モデル\n",
      "出典 : フリー百科事典『ウィキペディア（ Wikipedia ）』\n",
      "⼤規模⾔語モデル（だいきぼげんごモデル、英: large language model、LLM）は、多数のパラメータ（数千\n",
      "万から数⼗億）を持つ⼈⼯ニューラルネットワークで構成されるコンピュータ⾔語モデルで、膨⼤なラベル\n",
      "なしテキストを使⽤して⾃⼰教師あり学習または半教師あり学習によって訓練が⾏われる[1]。\n",
      "LLM は 2018 年頃に登場し、さまざまなタスク（仕事）で優れた性能を発揮している。これにより、⾃然⾔語\n",
      "処理の研究の焦点は、特定のタスクに特化した教師ありモデルを訓練するという以前のパラダイムから転換\n",
      "した[2]。⼤規模⾔語モデルの応⽤は⽬覚ましい成果を上げているが、⼤規模⾔語モデルの開発はまだ始まっ\n",
      "たばかりであり、多くの研究者が⼤規模⾔語モデルの改良に貢献している[3]。\n",
      "⼤規模⾔語モデルという⽤語の正式な定義はないが、⼤規模コーパスで事前訓練された、数百万から数⼗億\n",
      "以上のパラメータを持つディープラーニングモデルを指すことが多い。 LLM は、特定のタスク（感情分析、\n",
      "固有表現抽出、数学的推論など）のために訓練されたものとは異なり、幅広いタスクに優れた汎⽤モデルで\n",
      "ある[2][4]。 LLM がタスクを実⾏する能⼒や対応可能な範囲は、ある意味では設計における画期的な進歩には\n",
      "依存せず、 LLM に費やされた資源（データ、パラメータサイズ、計算⼒）の量の関数であるように⾒え\n",
      "る[5]。多数のパラメータを持ったニューラル⾔語モデルは、⽂の次の単語を予測するという単純なタスクで\n",
      "⼗分に訓練することで、⼈間の⾔葉の構⽂や意味の多くを捉えられることがわかった。さらに、⼤規模な⾔\n",
      "語モデルは、世の中に関するかなりの⼀般知識を⽰し、訓練中に⼤量の事実を「記憶」することができ\n",
      "る[2]。\n",
      "質の⾼い証拠とされる 2023 年のメタ分析によれば、⼤規模⾔語モデルの創造性に⽬を輝かせる研究者はもち\n",
      "ろん世界中に存在し、⼩規模⾔語モデルにはできないタスクで⼤規模⾔語モデルが創造的であると主張する\n",
      "学者もいるが、これは測定基準の選択によるものであり、創造性によるものではないことが⽰唆されてい\n",
      "る。異なる測定基準を選択した場合、⼤規模⾔語モデルの創造性の優位性は⾒られない可能性が⽰唆されて'\n",
      "-----\n",
      "page_content='性能を犠牲にす\n",
      "ることなく、財\n",
      "務タスクで既存\n",
      "モデルを⼤幅に\n",
      "上回る」とされ\n",
      "る\n",
      "PanGu-Σ 2023年3⽉ Huawei 1.085兆 3,290億トー\n",
      "クン[130]\n",
      "プロプライエタ\n",
      "リ\n",
      "OpenAssistant[131] 2023年3⽉ LAION 17 billion 1.5 trillion\n",
      "tokens Apache 2.0\n",
      "Trained on\n",
      "crowdsourced\n",
      "open data\n",
      "PaLM 2 (Pathways\n",
      "Language Model\n",
      "2)\n",
      "2023年5⽉ Google 340\n",
      "billion[132]\n",
      "3.6 trillion\n",
      "tokens[132] Proprietary Used in Bard\n",
      "chatbot.[133]\n",
      "RedPajama 2023年5⽉ Together\n",
      "Computer他 7 billion 1.2兆 Apache 2.0 LLaMAベース\n",
      "MPT 2023年5⽉ MosaicML\n",
      "Foundation 7 billion 1兆 Apache 2.0\n",
      "Mistral 2023年9⽉ Mistral AI 7 billion ? Apache 2.0\n",
      "脚注1. ^ モデルのアーキテクチャを説明する⽂書が最初に公開された⽇。\n",
      "2. ^ 多くの場合、研究者はサイズの異なる複数のモデルを公開または報告する。こうした場合、ここでは⼀\n",
      "番⼤きなモデルのサイズを記載している。\n",
      "3. ^ これは、事前学習されたモデルウェイトのライセンスである。たいていの場合、訓練コード⾃体はオー\n",
      "プンソースであるか、簡単に複製することができる。\n",
      "4. ^ 66Bを含めた⼩規模モデルは⼀般に公開されており、175Bのモデルはリクエストに応じて⼊⼿可能で\n",
      "ある。\n",
      "5. ^ 数学的な内容でフィルタリングされたウェブページおよびarXivプレプリントサーバーに投稿された論\n",
      "⽂からの385億トークン。\n",
      "6. ^ Facebookのライセンスと配布スキームにより、モデルへのアクセスは承認された研究者にが制限され\n",
      "ていたが、モデルウェイトが流出して広く利⽤されるようになった。\n",
      "7. ^ テクニカルレポートに述べられているように『GPT-4のような⼤規模モデルの市場競争と安全性への影'\n",
      "-----\n",
      "page_content='の不快な内容の特定、およびスワヒリ語のことわざに相当する英語の⽣\n",
      "成などがある[16]。\n",
      "Schaeffer らは、創発的な能⼒は予測不可能な形で獲得されるのではなく、滑らかなスケーリング則に従って\n",
      "予測通りに獲得されると主張している[17]。著者らは、 LLM が多肢選択問題を解く統計的トイモデルを検討\n",
      "し、他の種類のタスクを考慮して修正されたこの統計モデルが、これらのタスクにも適⽤できることを⽰し\n",
      "た。\n",
      "ここで、 をパラメータ数、 をモデルの性能とする。\n",
      "のとき、  は指数曲線（1でプラトーに達する前）となり、創発\n",
      "のように⾒える。\n",
      " のとき、  のプロットは直線（0でプラトーに達する前）\n",
      "となり、創発には⾒えない。\n",
      "創発的能⼒ のとき、  はステップ関数となり、創発の\n",
      "ように⾒える。\n",
      "⼤規模⾔語モデルの基本的な考え⽅は、単純で反復的なアーキテクチャを持つランダムな重みを持つニュー\n",
      "ラルネットワークを出発点とし、⼤規模な⾔語コーパスで訓練することである。\n",
      "この最も初期の例のひとつがエルマンネットワークで[18]、「⽝が男を追いかける」のような単純な⽂でリカ\n",
      "レントネットワークを訓練した。訓練したネットワークは、各単語をベクトル（内部表現）に変換した。次\n",
      "にこれらのベクトルを接近度によって⽊構造にクラスタリングした。その結果、ツリーはある構造を⽰すこ\n",
      "とがわかった。動詞と名詞はそれぞれ別の⼤きなクラスターに属していた。名詞のクラスター内には、無⽣\n",
      "物（ inanimates ）と⽣物（ animates ）の 2 つの⼩さなクラスターがある、などである。\n",
      "別の⽅法として、⾃然⾔語理解を記号プログラムによってコンピュータにプログラムする論理 AIがあった。\n",
      "この⽅法は 1990 年代まで主流であった。単純な機構と⼤規模なコーパスによって⾃然⾔語を学習するという\n",
      "着想は 1950 年代に始まったが、商業的に最初に成功したのは、統計的機械翻訳のためのIBM アライメントモ\n",
      "デル（ 1990 年代）であった。\n",
      "初期の「⼤規模」⾔語モデルは、⻑期・短期記憶（ LSTM 、 1997 年）などのリカレントアーキテクチャを使\n",
      "⽤して構築された。AlexNet（ 2012 年）が画像認識における⼤規模ニューラルネットワークの有効性を実証'\n",
      "-----\n",
      "page_content='科学的なテキス\n",
      "トや⽅法の訓練\n",
      "を受けている\n",
      "AlexaTM (Teacher\n",
      "Models) 2022年11⽉ Amazon 200\n",
      "億[121] 1.3兆[122] public web\n",
      "API[123]\n",
      "双⽅向のシーケ\n",
      "ンスからシーケ\n",
      "ンスへのアーキ\n",
      "テクチャ\n",
      "LLaMA (Large\n",
      "Language Model\n",
      "Meta AI)\n",
      "2023年2⽉ Meta 650\n",
      "億[124]\n",
      "1.4兆[124] Non-\n",
      "commercial\n",
      "research[注釈 6]\n",
      "20⾔語の⼤規\n",
      "模コーパスで訓\n",
      "練し、より少な\n",
      "いパラメータで\n",
      "の性能向上を⽬\n",
      "指す[124]。ス\n",
      "タンフォード⼤\n",
      "学の研究者は、\n",
      "Alpacaと呼ば\n",
      "れるLLaMAの名称 公開⽇[注釈 1] 開発者\n",
      "パラメー\n",
      "タ\n",
      "数[注釈 2]\n",
      "コーパスサ\n",
      "イズ\n",
      "ライセン\n",
      "ス[注釈 3] 注記\n",
      "重みに基づいて\n",
      "微調整されたモ\n",
      "デルを訓練し\n",
      "た[125]。\n",
      "GPT-4 2023年3⽉ OpenAI ⾮公\n",
      "開[注釈 7] ⾮公開 public web API\n",
      "ChatGPT Plus\n",
      "ユーザが利⽤で\n",
      "き、いくつかの\n",
      "製品で使⽤され\n",
      "ている\n",
      "Cerebras-GPT 2023年3⽉ Cerebras 130\n",
      "億[127] Apache 2.0 Chinchilla⽅式\n",
      "で訓練された\n",
      "Falcon 2023年3⽉\n",
      "Technology\n",
      "Innovation\n",
      "Institute\n",
      "1800\n",
      "億[128]\n",
      "3.5兆トーク\n",
      "ン[128]\n",
      "Falcon 180B\n",
      "TII License\n",
      "(Apache 2.0ベ\n",
      "ース)[128]\n",
      "モデルはGPT-3\n",
      "の75%、\n",
      "Chinchillaの\n",
      "40%、PaLM-\n",
      "62Bの80%の\n",
      "訓練計算量で済\n",
      "むとされる\n",
      "BloombergGPT 2023年3⽉ Bloomberg\n",
      "L.P. 500億\n",
      "3,630億トー\n",
      "ク\n",
      "ン[注釈 8][129]\n",
      "プロプライエタ\n",
      "リ\n",
      "独⾃ソースによ\n",
      "る財務データで\n",
      "訓練され、「⼀\n",
      "般的なLLMベ\n",
      "ンチマークでの\n",
      "性能を犠牲にす\n",
      "ることなく、財\n",
      "務タスクで既存\n",
      "モデルを⼤幅に\n",
      "上回る」とされ\n",
      "る\n",
      "PanGu-Σ 2023年3⽉ Huawei 1.085兆 3,290億トー\n",
      "クン[130]\n",
      "プロプライエタ\n",
      "リ'\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# 類似文書の取得\n",
    "similar_docs = retriever.get_relevant_documents(\"言語モデル\", k=3)\n",
    "# similar_docs = retriever.invoke(\"言語モデル\", k=3)\n",
    "\n",
    "# 類似文書を表示\n",
    "for doc in similar_docs:\n",
    "    print(doc)\n",
    "    print(\"-----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
